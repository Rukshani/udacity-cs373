(tracking
 (kalman filter
  (continuous)
  (uni-modal))
 (monte-carlo
  (discrete)
  (multi-modal)))
(gaussian
 (sigma-square aka variance
  (measure of uncertainty)
  (measure of width)
  (measure of relation between x and y))
 (mu aka mean
  (center of spread)))
(unit 2
 (14 measurement and motion 1
  (kalman filter
   (measurements and movements)
   (question-answer
    (question
     (measurement requires product or convolution?)
     (movement requires convolution or product?))
    (answer
     (product)
     (convolution)))))
 (15 measurement and motion 2
  (question-answer
   (bayes rule and total probability)
   (question-answer
    (question
     (measurement requires bayes rule and/or total probability?)
     (motion requires bayes rule and/or total probability?))
    (answer
     (measurement requires bayes rule)
     (motion requires total probability)))))
 (16 shifting the mean
  (kalman filters
   (measurement update
    (bayes rule)
    (product))
   (prediction
    (total probability)
    (convolution)))
  (question-answer
   (question
    (where will the new mean be after a measurement that has a
     higher mean and a smaller variance?
     (same as old mean?)
     (between old and new mean?)
     (higher than the new mean?)))
   (answer
    (in between the two means
     (slightly closer to the new mean, since its covariance is smaller)))))
 (17 predicting the peak
  (question-answer
   (question
    (where should the peak be after this measuring?
     (higher than the measurement peak?)
     (in between the measurement peak and the old peak?)
     (below both peaks?)))
   (answer
    (higher than the new peak
     (the resulting gaussian is more certain than the
      component guassians)
     (the two gaussians together have a higher information content)))))
 (18 parameter update
  (suppose we multiply two guassians as in bayes rule
   (prior mu and sigma-square)
   (measurement nu and r-square)
   (result mu-prime and sigma-square-prime)
   (result given by:
    (= mu-prime
       (/ (+ (* r-square mu) (* sigma-square nu))
          (+ r-square mu-square)))
    (= sigma-sqaure-prime
       (/ 1 (+ (/ 1 sigma-square)
               (/ 1 r-square)))))
   (question-answer
    (question
     (suppose we have
      (= mu 10)
      (= nu 12)
      (= sigma-square 4)
      (= r-square 4))
     (what are mu-prime and sigma-square-prime?))
    (answer
     (mu-prime is 11
      (since both weights are equal, it's just half way in between))
     (sigma-square-prime is 2
      (information content from both gaussians doubles,
       so resulting guassian width halves))))))
 (19 parameter update 2
  (let's DO THIS!!! again
   (question-answer
    (question
     (suppose we have
      (= mu 10)
      (= nu 13)
      (= sigma-square 8)
      (= r-square 2))
     (what are mu-prime and sigma-square-prime?))
    (answer
     (= mu-prime 62/5)
     (= sigma-square-prime 8/5)))))
 (20 separated gaussians
  (question-answer
   (question
    (suppose we have a prior and measurement that are reaally far apar
     from each other
     (will the new mean be
      (closer to the prior)
      (in between)
      (closer to the measure)
      (beyond the measure a little bit))?))
   (answer
    (it's in the middle
     (because the varainces are the same, so just average the means)))))
 (21 separated gaussians 2
  (hard question)
  (question-answer
   (question
    (will it the variance of the new gaussian be
     (smaller)
     (same size)
     (larger))?)
   (answer
    (smaller variance
     (more peaky gaussian)))))
 (22 gaussian motion
  (step back and look at what we achieved:
   (measure ment update
    (bayes rule)
    (multiplication))
   (motion update aka prediction
    (total probability)
    (addition)
    (really really easy step
     (start somewhere, move with some uncertainty
      (= mu-prime (+ mu nu))
      (= sigma-square-prime (+ sigma-square r-square))
      (r-square is motion uncertainty, aka variance)
      (nu is most likely motion)))))
  (question-answer
   (question
    (suppose we have
     (= mu 8)
     (= sigma-square 4)
     (= nu 10)
     (= r-square 6)
     what's the value of mu-prime and sigma-square-prime)?)
   (answer
    (= mu-prime (+ mu nu) (+ 8 10) 18)
    (= sigma-square-prime (+ sigma-square r-square) (+ 4 6) 10))))
 (25 kalman filter code
  (let's do it!  put into practice some measurements and motions
   (first update based on measurement, then on motion)
   (this is full kalman filter in 1D)))
 (26 kalman prediction
  (1D-kalman filter
   (you've programmed one)
   (incorporated motion, measurement))
  (many Ds
   (suppose you have 2D state space, like camera image)
   (2D-kalman filter affords you something amazing)
   (question-answer
    (question
     (suppose you see object going up and to the right)
     (where do you expect to see it next?))
    (answer
     (up and to the right about the same distance as it traveled before)
     (kalman filter allows you to implicitly estimate velocity)
     (sensor only sees position)
     (can predict future position)
     (really really really great)))))
 (27 kalman filter land
  (multivariate gaussians aka high dimension gaussian
   (mean is now a vector with an element per dimension)
   (variance is a matrix with D rows and D columns,
    where dimensionality = D)
   (there's a formula, but don't worry about it)
   (you can draw the contour lines of the gaussian
    (if it's tilted, then dimensions are correlated,
     getting information about x gives you an estimate for y))
   (1-D motion example
    (when
     (t=1, x=1)
     (t=2, x=2)
     (t=3, x=3)
     (t=4, predict x=4)
     you can infer from discrete locations a velocity)
    (how does kalman filter address this?
     (this is the true beauty of kalman filter)))))
 (28 kalman filter prediction
  (build 2-D estimate for 1-D prior example
   (1 d for position)
   (1 d for velocity)
   (if i know my initial position and not my velocity,
    have elongated gaussian elongated along velocity dimension)
   (there is a startling correlation between velocity and dimension))
  (question-answer
   (question
    (assume velocity 1 and position 1,
     where would the gaussian be centered in the next time step)?)
   (answer
    (predict position 2, velocity 1))))
 (29 another prediction
  (question-answer
   (question
    (consider velocity of 2, where would we expect next location to be)?)
   (answer
    (position 3, velocity 2))))
 (30 more kalman filters
  (gaussian from previous 2 videos has interesting property
   (x and y are highly correlated)
   (can't predict from single observation
    (location)
    (velocity))
   (but if we get another location,
    this gaussian gives us the velocity)
   (if we get our velocity,
    this gaussian gives us the location)
   (we've learned a lot about the relation between location and velocity))
  (big lesson
   (kalman filter states
    (observables
     (position))
    (hidden
     (velocity))
    (from multiple observations, we can estimate hidden))))
 (31 kalman filter design
  (design kf
   (need 2 things
    (for state
     (need state transition function
      (usually matrix
       (example ((1 1)
                 (0 1)) multiply this matrix by vector (position velocity)))))
    (for measurements
     (need measurement function
      (example (new-location = old-location + velocity after timestep))
       called H))))
  (update
   (prediction
    (x = estimate)
    (P = uncertainty covariance)
    (F = state transition matrix)
    (u = motion vector)
    (x-prime = F x + mu)
    (p-prime = F dot P dot F-transpose))
   (measurement update
    (z = measurement)
    (H = measurement function)
    (R = measurement noise)
    (y = z - H * x)
    (S = H dot P dot H-transpose + R)
    (K = P dot H-transpose dot S-inverse (called "kalman gain"))
    (x-prime = x + (K dot y))
    (P-prime = (I - K dot H) dot P))))
 (32 kalman matrices
  (challenging programming assignment, should take a while)
  (implement multi-d kalman filter)
  (matrix class
   (initializing matrices)
   (inverse)
   (multiplication)
   (show))
  (do measurement/observation first, then do prediction))
 (33 the next class
  (learn how to program a particle filter))
 (34 conclusion
  (ace the hw!)
  (next week: particle filters, third way to estimate state)))
